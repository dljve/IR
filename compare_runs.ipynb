{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "from operator import itemgetter\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from msmarco_compare import compute_metrics_from_files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metrics(file):\n",
    "    metrics = {}\n",
    "    with open(file, 'r') as f:\n",
    "        for line in f:\n",
    "            metric, qid, score = line.split('\\t')\n",
    "            metric = metric.strip()\n",
    "            qid = qid.strip()\n",
    "            score = score.strip()\n",
    "            if qid == 'all':\n",
    "                continue\n",
    "            if metric not in metrics:\n",
    "                metrics[metric] = {}\n",
    "            metrics[metric][qid] = float(score)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(all_results, ymin=-1, ymax=1, output_path=\".\"):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(16, 3))\n",
    "    all_results.sort(key = itemgetter(1), reverse=True)\n",
    "    x = [_x+0.5 for _x in range(len(all_results))]\n",
    "    y = [float(ele[1]) for ele in all_results]\n",
    "    ax.bar(x, y, width=0.6, align='edge')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([int(ele[0]) for ele in all_results], {'fontsize': 4}, rotation='vertical')\n",
    "    ax.grid(True)\n",
    "    ax.set_title(\"Per-topic analysis on {}\".format(metric))\n",
    "    ax.set_xlabel('Topics')\n",
    "    ax.set_ylabel('{} Diff'.format(metric))\n",
    "    ax.set_ylim(ymin, ymax)\n",
    "    output_fn = os.path.join(output_path, 'per_query_{}.pdf'.format(metric))\n",
    "    plt.savefig(output_fn, bbox_inches='tight', format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1037798\t0.0\t0.0\t0.0\n",
      "104861\t0.0091\t0.0091\t0.0\n",
      "1063750\t0.0076\t0.0074\t-0.0002\n",
      "1103812\t0.0292\t0.0292\t0.0\n",
      "1106007\t0.0041\t0.0041\t0.0\n",
      "1110199\t0.0711\t0.0711\t0.0\n",
      "1112341\t0.0056\t0.007\t0.0014\n",
      "1113437\t0.0067\t0.0067\t0.0\n",
      "1114646\t0.1019\t0.1019\t0.0\n",
      "1114819\t0.0158\t0.0158\t0.0\n",
      "1115776\t0.6092\t0.5687\t-0.0405\n",
      "1117099\t0.011\t0.0125\t0.0015\n",
      "1121402\t0.1818\t0.1818\t0.0\n",
      "1124210\t0.0362\t0.0362\t0.0\n",
      "1129237\t0.0614\t0.0526\t-0.0088\n",
      "1132213\t0.3366\t0.3366\t0.0\n",
      "1133167\t0.0503\t0.0503\t0.0\n",
      "130510\t0.2063\t0.2063\t0.0\n",
      "131843\t0.24\t0.28\t0.04\n",
      "146187\t0.177\t0.177\t0.0\n",
      "148538\t0.0257\t0.0257\t0.0\n",
      "156493\t0.0662\t0.0662\t0.0\n",
      "182539\t0.1626\t0.1626\t0.0\n",
      "183378\t0.0241\t0.0241\t0.0\n",
      "19335\t0.1016\t0.1016\t0.0\n",
      "207786\t0.085\t0.085\t0.0\n",
      "264014\t0.0565\t0.0565\t0.0\n",
      "287683\t0.3278\t0.3417\t0.0139\n",
      "359349\t0.0546\t0.0546\t0.0\n",
      "405717\t0.1124\t0.1124\t0.0\n",
      "443396\t0.0\t0.0\t0.0\n",
      "451602\t0.0059\t0.0059\t0.0\n",
      "47923\t0.0071\t0.0071\t0.0\n",
      "489204\t0.0029\t0.0029\t0.0\n",
      "490595\t0.1293\t0.1293\t0.0\n",
      "527433\t0.0683\t0.0683\t0.0\n",
      "573724\t0.14\t0.1448\t0.0048\n",
      "833860\t0.0348\t0.0348\t0.0\n",
      "855410\t1.0\t1.0\t0.0\n",
      "87181\t0.0421\t0.0421\t0.0\n",
      "87452\t0.0539\t0.0539\t0.0\n",
      "915593\t0.0048\t0.0063\t0.0015\n",
      "962179\t0.3662\t0.3662\t0.0\n",
      "base mean: 0.117\n",
      "comp mean: 0.1174\n",
      "t-statistic: -0.226029, p-value: 0.822275\n",
      "better (diff > 0.01):   2\n",
      "worse  (diff > 0.01):   1\n",
      "(mostly) unchanged  :  40\n",
      "biggest gain: 0.04 (topic 131843)\n",
      "biggest loss: -0.0405 (topic 1115776)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-c50c7d68fef2>:8: MatplotlibDeprecationWarning: Passing the fontdict parameter of _set_ticklabels() positionally is deprecated since Matplotlib 3.3; the parameter will become keyword-only two minor releases later.\n",
      "  ax.set_xticklabels([int(ele[0]) for ele in all_results], {'fontsize': 4}, rotation='vertical')\n"
     ]
    }
   ],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--base\", type=str, help='base run', required=True)\n",
    "# parser.add_argument(\"--comparison\", type=str, help='comparison run', required=True)\n",
    "# parser.add_argument(\"--qrels\", type=str, help='qrels', required=True)\n",
    "# parser.add_argument(\"--metric\", type=str, help='metric', default=\"map\")\n",
    "# parser.add_argument(\"--msmarco\", action='store_true', default=False, help='whether to use masarco eval script')\n",
    "# parser.add_argument(\"--ymin\", type=float, help='min value of the y axis', default=-1)\n",
    "# parser.add_argument(\"--ymax\", type=float, help='max value of the y axis', default=1)\n",
    "\n",
    "#args = parser.parse_args()\n",
    "\n",
    "base = \"runs/BM25.txt\"\n",
    "comp = \"runs/word2vec.txt\"\n",
    "qrels = \"qrels.dl19-doc.txt\"\n",
    "#metric = \"map set_f.1 P.10 set_recall.10 ndcg\"\n",
    "metric = \"map\"\n",
    "msmarco = False\n",
    "ymin = -1\n",
    "ymax = 1\n",
    "\n",
    "if msmarco:\n",
    "    base_all, base_metrics = compute_metrics_from_files(qrels, base, per_query_score=True) \n",
    "    comp_all, comp_metrics = compute_metrics_from_files(qrels, comp, per_query_score=True) \n",
    "else:\n",
    "    os.system(f'G:/python/ir/eval/trec_eval.9.0.4/trec_eval -q -M1000 -m {metric} {qrels} {base} > eval.base')\n",
    "    os.system(f'G:/python/ir/eval/trec_eval.9.0.4/trec_eval -q -M1000 -m {metric} {qrels} {comp} > eval.comp')\n",
    "\n",
    "    base_metrics = load_metrics('eval.base')\n",
    "    comp_metrics = load_metrics('eval.comp')\n",
    "\n",
    "# trec_eval expects something like 'P.10' on the command line but outputs 'P_10'\n",
    "if \".\" in metric:\n",
    "    metric = \"_\".join(metric.split(\".\"))\n",
    "\n",
    "all_results = []\n",
    "num_better = 0\n",
    "num_worse = 0\n",
    "num_unchanged = 0\n",
    "biggest_gain = 0\n",
    "biggest_gain_topic = ''\n",
    "biggest_loss = 0\n",
    "biggest_loss_topic = ''\n",
    "if msmarco:\n",
    "    metric = \"MRR@10\"\n",
    "keys = []\n",
    "for key in base_metrics[metric]:\n",
    "    base_score = base_metrics[metric][key]\n",
    "    if key not in comp_metrics[metric]:\n",
    "        continue\n",
    "    keys.append(key)\n",
    "    comp_score = comp_metrics[metric][key]\n",
    "    diff = comp_score - base_score\n",
    "    # This is our relatively arbitrary definition of \"better\", \"worse\", and \"unchanged\".\n",
    "    if diff > 0.01:\n",
    "        num_better += 1\n",
    "    elif diff < -0.01:\n",
    "        num_worse += 1\n",
    "    else:\n",
    "        num_unchanged += 1\n",
    "    if diff > biggest_gain:\n",
    "        biggest_gain = diff\n",
    "        biggest_gain_topic = key\n",
    "    if diff < biggest_loss:\n",
    "        biggest_loss = diff\n",
    "        biggest_loss_topic = key\n",
    "    all_results.append((key, diff))\n",
    "    print(f'{key}\\t{base_score:.4}\\t{comp_score:.4}\\t{diff:.4}')\n",
    "\n",
    "# Extract the paired scores\n",
    "a = [base_metrics[metric][k] for k in keys]\n",
    "b = [comp_metrics[metric][k] for k in keys]\n",
    "\n",
    "(tstat, pvalue) = scipy.stats.ttest_rel(a, b)\n",
    "print(f'base mean: {np.mean(a):.4}')\n",
    "print(f'comp mean: {np.mean(b):.4}')\n",
    "print(f't-statistic: {tstat:.6}, p-value: {pvalue:.6}')\n",
    "print(f'better (diff > 0.01): {num_better:>3}')\n",
    "print(f'worse  (diff > 0.01): {num_worse:>3}')\n",
    "print(f'(mostly) unchanged  : {num_unchanged:>3}')\n",
    "print(f'biggest gain: {biggest_gain:.4} (topic {biggest_gain_topic})')\n",
    "print(f'biggest loss: {biggest_loss:.4} (topic {biggest_loss_topic})')\n",
    "\n",
    "plot(all_results, ymin=ymin, ymax=ymax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
